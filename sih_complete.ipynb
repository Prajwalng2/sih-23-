{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtX982uaZCS0lw3VM8wuyW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajwalng2/sih-23-complete/blob/main/sih_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBQjMVxcn9Xw"
      },
      "outputs": [],
      "source": [
        "\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/api/chatbot', methods=['POST'])\n",
        "def chatbot():\n",
        "\n",
        "    user_query = request.json['query']\n",
        "\n",
        "\n",
        "    bot_response = \"This is a response from the chatbot.\"\n",
        "\n",
        "\n",
        "    return jsonify({'response': bot_response})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=8080)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "// Frontend (Next.js)\n",
        "import { useState } from 'react';\n",
        "\n",
        "const Chatbot = () => {\n",
        "  const [userQuery, setUserQuery] = useState('');\n",
        "  const [botResponse, setBotResponse] = useState('');\n",
        "\n",
        "  const handleUserQuery = async () => {\n",
        "    try {\n",
        "      const response = await fetch('/api/chatbot', {\n",
        "        method: 'POST',\n",
        "        headers: {\n",
        "          'Content-Type': 'application/json',\n",
        "        },\n",
        "        body: JSON.stringify({ query: userQuery }),\n",
        "      });\n",
        "      const data = await response.json();\n",
        "      setBotResponse(data.response);\n",
        "    } catch (error) {\n",
        "      console.error('Error:', error);\n",
        "    }\n",
        "  };\n",
        "\n",
        "  return (\n",
        "    <div>\n",
        "      <input\n",
        "        type=\"text\"\n",
        "        value={userQuery}\n",
        "        onChange={(e) => setUserQuery(e.target.value)}\n",
        "      />\n",
        "      <button onClick={handleUserQuery}>Send</button>\n",
        "      <div>\n",
        "        <p>User: {userQuery}</p>\n",
        "        <p>Bot: {botResponse}</p>\n",
        "      </div>\n",
        "    </div>\n",
        "  );\n",
        "};\n",
        "\n",
        "export default Chatbot;\n"
      ],
      "metadata": {
        "id": "abYs65yerULC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "public_url = ngrok.connect(port='5000')\n",
        "\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}/\\\"\".format(public_url, 5000))\n"
      ],
      "metadata": {
        "id": "xIieIyCpraqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import { useState } from 'react';\n",
        "\n",
        "function Chatbot() {\n",
        "  const [userMessage, setUserMessage] = useState('');\n",
        "  const [botResponse, setBotResponse] = useState('');\n",
        "\n",
        "  const sendUserMessage = async () => {\n",
        "    const response = await fetch('YOUR_NGROK_PUBLIC_URL/chat', {\n",
        "      method: 'POST',\n",
        "      headers: {\n",
        "        'Content-Type': 'application/json',\n",
        "      },\n",
        "      body: JSON.stringify({ user_message: userMessage }),\n",
        "    });\n",
        "\n",
        "    const data = await response.json();\n",
        "    setBotResponse(data.bot_response);\n",
        "  };\n",
        "\n",
        "  return (\n",
        "    <div>\n",
        "      <div>{botResponse}</div>\n",
        "      <input\n",
        "        type=\"text\"\n",
        "        value={userMessage}\n",
        "        onChange={(e) => setUserMessage(e.target.value)}\n",
        "      />\n",
        "      <button onClick={sendUserMessage}>Send</button>\n",
        "    </div>\n",
        "  );\n",
        "}\n",
        "\n",
        "export default Chatbot;\n"
      ],
      "metadata": {
        "id": "PYEsEcNosCqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install huggingface_hub\n",
        "!pip install sentence-transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "mdypZokAsRx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import textwrap\n",
        "import sys\n"
      ],
      "metadata": {
        "id": "TADC_2DvsU3z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "GRsdo_ZssXmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HuggingFaceHub_API_Token']= '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'"
      ],
      "metadata": {
        "id": "uMBYX_y9saOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "id": "lta2LhLrsvC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = UnstructuredFileLoader('/content/The-Mines-Act.csv')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "hWjCuVmEsySR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter=CharacterTextSplitter(separator='\\n',\n",
        "                                    chunk_size=1200,\n",
        "                                    chunk_overlap=50)\n",
        "text_chunks=text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "SL3VLvg_s2wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',model_kwargs={'device': 'cuda'})"
      ],
      "metadata": {
        "id": "KAlEUDn-s_8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "id": "90mq8AjYtIMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore=FAISS.from_documents(text_chunks, embeddings)"
      ],
      "metadata": {
        "id": "ZeN58rRbtOxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
      ],
      "metadata": {
        "id": "jkpv_qeOtXoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             use_auth_token=True,\n",
        "                                             load_in_8bit=Tru"
      ],
      "metadata": {
        "id": "DD7yUBF2tYMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                max_new_tokens = 1024,\n",
        "                do_sample=True,\n",
        "                top_k=10,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "                )"
      ],
      "metadata": {
        "id": "0nBpacuBtbdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})"
      ],
      "metadata": {
        "id": "DyPogF4xtejE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain =  RetrievalQA.from_chain_type(llm=llm, chain_type = \"stuff\",return_source_documents=True, retriever=vectorstore.as_retriever())"
      ],
      "metadata": {
        "id": "D15mxrfRthFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"Enter your query: \")\n",
        "result=chain({\"query\": query}, return_only_outputs=True)\n",
        "wrapped_text = textwrap.fill(result['result'], width=500)\n",
        "wrapped_text"
      ],
      "metadata": {
        "id": "9Rp232Rstj0F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}